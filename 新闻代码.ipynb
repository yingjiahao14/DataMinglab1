{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载2周的新闻数据（尽可能多） 进行学习过的数据预处理技术（重复记录，噪音数据清理等）\n",
    "# 分析并观察各种板块新闻数据的分布等\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.chinanews.com/\"\n",
    "html = requests.get(url)  \n",
    "soup = bs(html.content, \"html.parser\")\n",
    "url_scroll = soup.find(name='ul',attrs={\"class\":\"nav_navcon\"}).find('a')['href'] #获取顶部导航栏 排第一的滚动页面的url \n",
    "url_scroll = \"https:\" + url_scroll\n",
    "f = open('news.csv', \"a+\")\n",
    "for i in range(1,11):    \n",
    "    url_scroll = url_scroll[:-6] + str(i) + \".html\"  \n",
    "    html_scroll = requests.get(url_scroll)      \n",
    "    soup_scroll = bs(html_scroll.content,\"html.parser\")    \n",
    "    news = soup_scroll.find(name=\"div\", attrs={\"class\" :\"content_list\"})   \n",
    "    news = news.findAll(name='li')\n",
    "    for n in news:\n",
    "        news_lab = n.find(name='div', attrs={\"class\" :\"dd_lm\"})\n",
    "        if news_lab == None:\n",
    "            continue\n",
    "        #print(news_lab.text)\n",
    "        news_title = n.find(name='div', attrs={\"class\" :\"dd_bt\"}).text\n",
    "        #print(news_title)\n",
    "        news_time = n.find(name='div', attrs={\"class\":\"dd_time\"}).text\n",
    "        #print(news_time)\n",
    "        f.write(news_lab.text + \",\" + news_time + \",\" + news_title+ \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "all_news = [] \n",
    "url = \"http://www.chinanews.com/\"\n",
    "html = requests.get(url)  \n",
    "soup = bs(html.content, \"html.parser\")\n",
    "navbar = soup.find(name='ul',attrs={\"class\":\"nav_navcon\"}).findAll(\"a\")\n",
    "firstr = 'news1.csv'\n",
    "for nav in navbar[1:14]: # 取第一栏    \n",
    "    f = open(firstr,'a+',encoding=\"utf-8\")\n",
    "    if nav['href'][:5]!=\"http:\":          \n",
    "        url_nav = \"http:\" + nav['href']  # 获取改板块的url\n",
    "    html_nav = requests.get(url_nav)      \n",
    "    soup_nav = bs(html_nav.content,\"html.parser\") \n",
    "    for n in soup_nav.findAll(\"li\") + soup_nav.findAll(\"em\") + soup_nav.findAll(\"h1\"):  \n",
    "        if n.a and n.a.string and len(n.a.string) > 7:  \n",
    "            #print(n.a.string)\n",
    "            f.write(str(nav.string) + \",\" + str(n.a.string) + \"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n"
     ]
    }
   ],
   "source": [
    "print(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
